{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "9CLLEFhRfh-u",
    "outputId": "8104d839-f3a1-4cc8-c501-4b397f20b9cc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2, numpy as np, os\n",
    "import pandas as pd, time\n",
    "import tensorflow.contrib as tf_contrib\n",
    "\n",
    "DECAY_BN = 0.0001\n",
    "EPSILON_BN = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emnMJ4xzfh_D"
   },
   "outputs": [],
   "source": [
    "def read_images(path, ran, phase):\n",
    "    save_path = phase+\"_images.npy\"\n",
    "    if(os.path.exists(save_path)):\n",
    "        print(\"npy file available...\")\n",
    "        return np.load(save_path)\n",
    "    images = []\n",
    "    for r, d, f in os.walk(path):\n",
    "        for i in range(1, ran+1):\n",
    "            img_name = os.path.join(r, str(i)+\".jpg\")\n",
    "            images.append(cv2.imread(img_name)/255.)\n",
    "    print(\"done loading from \" + path)\n",
    "    images = np.array(images)\n",
    "    np.save(save_path, images)\n",
    "    return images\n",
    "\n",
    "def read_labels(csv_file, phase, nb_classes = 8):\n",
    "    \n",
    "    labels = np.array(pd.read_csv(csv_file, header = None).values)\n",
    "    labels = np.reshape(labels, (labels.shape[1], labels.shape[0]))\n",
    "    one_hot_targets = np.eye(nb_classes)[[i-1 for i in labels]]\n",
    "    one_hot_targets = np.reshape(one_hot_targets, (one_hot_targets.shape[0], 8))\n",
    "\n",
    "    return one_hot_targets\n",
    "    \n",
    "\n",
    "def get_num_trainable_params():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        total_parameters += variable_parameters\n",
    "    print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "81y1SW-Tfh_N",
    "outputId": "12b148a8-bdee-47a5-a3bd-cb1c8434f1b8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npy file available...\n",
      "npy file available...\n",
      "Train images: (1888, 256, 256, 3)\n",
      "Test images:  (800, 256, 256, 3)\n",
      "Train labels: (1888, 8)\n",
      "Test labels:  (800, 8)\n"
     ]
    }
   ],
   "source": [
    "train_images = read_images(\"../train/\", 1888, \"train\")\n",
    "test_images = read_images(\"../test/\", 800, \"test\")\n",
    "\n",
    "train_labels = read_labels(\"../train_labels.csv\", \"train\")\n",
    "test_labels = read_labels(\"../test_labels.csv\", \"test\")\n",
    "\n",
    "print(\"Train images: \" + str(train_images.shape))\n",
    "print(\"Test images:  \" + str(test_images.shape))\n",
    "\n",
    "print(\"Train labels: \" + str(train_labels.shape))\n",
    "print(\"Test labels:  \" + str(test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TzFod2kfh_Y"
   },
   "outputs": [],
   "source": [
    "def residual_block(x, filter_size, wt_init, weight_regularizer, conv_number, instance, is_train, stride = 1, reuse = False):\n",
    "    \n",
    "    prev = x\n",
    "    x = tf.layers.conv2d(x, filters=filter_size, kernel_size=3, kernel_initializer=wt_init, strides=stride,\n",
    "                                kernel_regularizer=weight_regularizer, padding=\"SAME\", trainable=is_train, name=\"shortcut_conv\"+conv_number+\"_\"+instance, reuse=reuse)\n",
    "    x = tf_contrib.layers.batch_norm(x, decay=DECAY_BN, epsilon=EPSILON_BN, center=True, scale=True, trainable=is_train, reuse=reuse, scope=\"short_batch_norm\"+conv_number+\"_\"+instance)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.conv2d(x, filters=filter_size, kernel_size=3, kernel_initializer=wt_init, trainable=is_train,\n",
    "                         kernel_regularizer=weight_regularizer, name=\"conv\"+conv_number+\"_\"+instance, padding=\"SAME\", reuse=reuse)\n",
    "    \n",
    "    x = tf_contrib.layers.batch_norm(x, decay=DECAY_BN, epsilon=EPSILON_BN, center=True, scale=True, trainable=is_train, reuse=reuse, scope=\"batch_norm\"+conv_number+\"_\"+instance)\n",
    "    \n",
    "    if(stride == 2):\n",
    "        #x = prev + x\n",
    "        prev = tf.layers.conv2d(prev, filters=filter_size, kernel_size=1, kernel_initializer=wt_init, strides=stride,\n",
    "                                kernel_regularizer=weight_regularizer, padding=\"SAME\", trainable=is_train, name=\"1x1_\"+conv_number+\"_\"+instance, reuse=reuse)\n",
    "    \n",
    "    x = prev + x\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_model(train_input, is_train = False, reuse = False):\n",
    "    wt_init = tf_contrib.layers.variance_scaling_initializer()\n",
    "    weight_regularizer = tf_contrib.layers.l2_regularizer(0.0001)\n",
    "    \n",
    "    x = tf.layers.conv2d(train_input, filters=64, kernel_size=7, kernel_initializer=wt_init, strides=2, \n",
    "                         kernel_regularizer=weight_regularizer, trainable=is_train, name=\"conv1\", padding='SAME', reuse=reuse)\n",
    "    x = tf_contrib.layers.batch_norm(x, decay=DECAY_BN, epsilon=EPSILON_BN, center=True, scale=True, trainable=is_train, reuse=reuse, scope=\"batch_norm1\")\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    x = tf.layers.max_pooling2d(x, pool_size = 2, strides = 2, name=\"maxpool\") # 56x56x64\n",
    "    \n",
    "    x = residual_block(x, 64, wt_init, weight_regularizer, \"2\", \"1\", is_train, reuse=reuse)\n",
    "    x = residual_block(x, 64, wt_init, weight_regularizer, \"2\", \"2\", is_train, reuse=reuse)\n",
    "    \n",
    "    x = residual_block(x, 128, wt_init, weight_regularizer, \"3\", \"1\", is_train, stride=2, reuse=reuse)\n",
    "    x = residual_block(x, 128, wt_init, weight_regularizer, \"3\", \"2\", is_train, reuse=reuse)\n",
    "    \n",
    "    x = residual_block(x, 256, wt_init, weight_regularizer, \"4\", \"1\", is_train, stride=2, reuse=reuse)\n",
    "    x = residual_block(x, 256, wt_init, weight_regularizer, \"4\", \"2\", is_train, reuse=reuse)\n",
    "    \n",
    "    x = residual_block(x, 512, wt_init, weight_regularizer, \"5\", \"1\", is_train, stride=2, reuse=reuse)\n",
    "    x = residual_block(x, 512, wt_init, weight_regularizer, \"5\", \"2\", is_train, reuse=reuse)\n",
    "    \n",
    "    x = tf.layers.average_pooling2d(x, pool_size=7, strides=2, name=\"avgpool\")\n",
    "    x = tf.contrib.layers.flatten(x)\n",
    "    \n",
    "    #x = tf.layers.dense(x, 1000, activation=tf.nn.relu, kernel_initializer=wt_init, trainable=is_train, name=\"fc1\")\n",
    "    x = tf.layers.dense(x, 8, kernel_initializer=wt_init, trainable=is_train, name=\"output\", reuse=reuse)\n",
    "    #x = tf.nn.softmax(x, name=\"output_logits\")\n",
    "    \n",
    "    return x\n",
    "\n",
    "def get_losses(output, label):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=label))\n",
    "    prediction = tf.equal(tf.argmax(output, -1), tf.argmax(label, -1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "-ivgxyHWfh_i",
    "outputId": "c8668f45-dba7-46eb-a4b5-bad59136001e"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_input = tf.placeholder(tf.float32, (None, 224, 224, 3), name = \"train_input\")\n",
    "train_label = tf.placeholder(tf.int32, (None, 8), name = \"train_label\")\n",
    "test_input = tf.placeholder(tf.float32, (None, 224, 224, 3), name = \"test_input\")\n",
    "test_label = tf.placeholder(tf.int32, (None, 8), name = \"test_label\")\n",
    "\n",
    "batch = 32\n",
    "\n",
    "lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "train_output = build_model(train_input, is_train = True)\n",
    "test_output = build_model(test_input, reuse = True)\n",
    "\n",
    "train_loss, train_acc = get_losses(train_output, train_label)\n",
    "test_loss, test_acc = get_losses(test_output, test_label)\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9).minimize(train_loss)\n",
    "\n",
    "#### Summary #####\n",
    "summary_train_loss = tf.summary.scalar(\"train_loss\", train_loss)\n",
    "summary_test_loss = tf.summary.scalar(\"train_loss\", test_loss)\n",
    "\n",
    "summary_train_accuracy = tf.summary.scalar(\"train_accuracy\", train_acc)\n",
    "summary_test_accuracy = tf.summary.scalar(\"train_accuracy\", test_acc)\n",
    "\n",
    "train_summary = tf.summary.merge([summary_train_loss, summary_train_accuracy])\n",
    "test_summary = tf.summary.merge([summary_test_loss, summary_test_accuracy])\n",
    "\n",
    "#get_num_trainable_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esBxY1LLfh_u",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_mini_batches(X, y, crop = True, batch_size = 32):\n",
    "    \n",
    "    n_minibatches = X.shape[0] // batch_size\n",
    "    data = list(zip(X, y))\n",
    "    np.random.shuffle(data)\n",
    "    X, y = zip(*data)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    mini_batches = []\n",
    "    i = 0\n",
    "    for i in range(n_minibatches):\n",
    "        X_mini = X[i * batch_size:(i + 1)*batch_size]\n",
    "        if(crop):\n",
    "            X_mini = data_augmentation(X_mini, 224)\n",
    "        y_mini = y[i * batch_size:(i + 1)*batch_size]\n",
    "        mini_batches.append((X_mini, y_mini))\n",
    "    if(X.shape[0] % batch_size != 0):\n",
    "        X_mini = X[i * batch_size:X.shape[0]]\n",
    "        if(crop):\n",
    "            X_mini = data_augmentation(X_mini, 224)\n",
    "        y_mini = y[i * batch_size:y.shape[0]]\n",
    "        mini_batches.append((X_mini, y_mini))\n",
    "        n_minibatches += 1\n",
    "    return mini_batches, n_minibatches\n",
    "\n",
    "def _random_crop(batch, crop_shape, padding=None):\n",
    "    \n",
    "    oshape = np.shape(batch[0])\n",
    "    if padding:\n",
    "        oshape = (oshape[0] + 2 * padding, oshape[1] + 2 * padding)\n",
    "    new_batch = []\n",
    "    npad = ((padding, padding), (padding, padding), (0, 0))\n",
    "    for i in range(len(batch)):\n",
    "        new_batch.append(batch[i])\n",
    "        if padding:\n",
    "            new_batch[i] = np.lib.pad(batch[i], pad_width=npad, mode='constant', constant_values=0)\n",
    "        nh = np.random.randint(0, oshape[0] - crop_shape[0])\n",
    "        nw = np.random.randint(0, oshape[1] - crop_shape[1])\n",
    "        new_batch[i] = new_batch[i][nh:nh + crop_shape[0], nw:nw + crop_shape[1]]\n",
    "    return new_batch\n",
    "        \n",
    "    \n",
    "def data_augmentation(batch, img_size):\n",
    "    return _random_crop(batch, [img_size, img_size, 3])\n",
    "\n",
    "\n",
    "aug_test_images = data_augmentation(test_images, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkMeKGJefh_3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting cell\n",
      "Starting Train!!!\n",
      "Epoch: [ 0] [    1/   59], train_accuracy: 0.16, learning_rate : 0.1000, train_loss: 4.23\n",
      "Epoch: [ 0] [    2/   59], train_accuracy: 0.16, learning_rate : 0.1000, train_loss: 39.27\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "epoch_lr = 0.1\n",
    "counter = 0\n",
    "test_counter = 0\n",
    "\n",
    "print(\"starting cell\")\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "writer = tf.summary.FileWriter(\"./logs\", sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "test_feed_dict = {test_input: aug_test_images, test_label: test_labels}\n",
    "\n",
    "print(\"Starting Train!!!\")\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    if epoch == int(epochs * 0.5) or epoch == int(epochs * 0.75):\n",
    "        epoch_lr = epoch_lr * 0.1\n",
    "    minibatches, n_batches = get_mini_batches(train_images, train_labels, batch_size = batch)\n",
    "    i = 0\n",
    "    for minibatch in minibatches:\n",
    "        i += 1\n",
    "        X, y = minibatch\n",
    "        \n",
    "        train_feed_dict = {train_input:X, train_label: y, lr: epoch_lr}\n",
    "        \n",
    "        acc, _, l, summary1 = sess.run([train_acc, optimizer, train_loss, train_summary], feed_dict=train_feed_dict)\n",
    "        writer.add_summary(summary1, counter)\n",
    "        counter += 1\n",
    "        \n",
    "        print(\"Epoch: [%2d] [%5d/%5d], train_accuracy: %.2f, learning_rate : %.4f, train_loss: %.2f\" \\\n",
    "                      % (epoch, i, n_batches, acc, epoch_lr, l))\n",
    "    if(epoch%5 == 0):\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver.save(sess, os.path.join(checkpoint_dir, 'ResNet18.model'), global_step=counter)\n",
    "        \n",
    "    t_acc, summary2, t_loss = sess.run([test_acc, test_summary, test_loss], feed_dict=test_feed_dict)\n",
    "    writer.add_summary(summary2, test_counter)\n",
    "    print(\"Epoch: [%2d], test_accuracy: %.2f, test_loss: %.2f\"%(epoch, t_acc, t_loss))\n",
    "    test_counter += 1\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QlAdeSnMfh__"
   },
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# out = sess.run(train_output, feed_dict={train_input: data_augmentation(test_images[0:15], 224)})\n",
    "\n",
    "# print(out)\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hCD-BwQVfiAE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-HtXbKFfiAJ"
   },
   "outputs": [],
   "source": [
    "# mini, n = get_mini_batches(train_images[0:15], train_labels[0:15])\n",
    "# for i in mini:\n",
    "#     x, y = i\n",
    "#     break\n",
    "    \n",
    "# x = np.array(x)\n",
    "# y = np.array(y)\n",
    "\n",
    "# idx = 2\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(x[idx])\n",
    "# y[idx]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet18.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
